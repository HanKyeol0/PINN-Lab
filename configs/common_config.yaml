seed: 42
device: "cuda"            # "cuda" or "cpu"
train:
  epochs: 500
  batch:
    n_f: 4096             # collocation points
    n_b: 1024             # boundary points
    n_0: 1024             # initial points (if applicable)
  optimizer:
    name: adam
    lr: 1.0e-3
  early_stopping:
    enabled: true
    patience: 1000
    min_delta: 1.0e-5
  loss_weights:
    f: 1.0    # PDE residual
    b: 10.0   # boundary
    ic: 10.0  # initial
log:
  out_dir: "outputs"
  wandb:
    enabled: true
    project: "pinnlab"
    entity: null      # or your team
    mode: "online"      # "auto" | "offline" | "disabled"
eval:
  grid:
    nx: 200
    ny: 200
    nt: 100
gradflow:
  enabled: true
  every: 200
  store_vectors: true    # must be true if you want KDE
  wandb_hist: false
  max_keep: 20  # how many vectors to keep
  stop_at: 10000
  plot:
    enabled: true          # <-- toggle all plotting
    kde: true              # KDE of latest vectors per layer (paper-style) (KDE: kernel density estimation)
    stats: true            # time-series curves of mean/max/L2 per layer
    losses: ["bc", "res"]  # which losses to compare
    loss_kde: true
    loss_kde_filename: "gradflow_kde_losses.png"
    kde_filename: "gradflow_kde.png"
    stats_prefix: "gradflow_stats"
    max_cols: 4            # subplots per row
    tagging:
      # Regex applied to full module names
      # (keep them permissive; they only help classification)
      main_regex: "(?:^|\\.)(?:main|central|body)(?:\\.|$)"
      skip_regex: "(?:^|\\.)(?:skip|shortcut|proj|downsample)(?:\\.|$)"
finetune:
  lora:
    enabled: false     # true for LoRA run
    r: 8
    alpha: 16.0
    dropout: 0.0
    train_base: false  # freeze base weights during LoRA